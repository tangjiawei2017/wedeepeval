---
id: tutorial-summarization-evaluation
title: Evaluating Your Summarizer
sidebar_label: Evaluate Your Summarizer
---

In the previous section, we built a meeting summarization agent and reviewed the output it generated from a sample conversation transcript.
But how do we assess the quality of that output? Many developers tend to eyeball the results of their LLM applications. This is a common and significant issue in LLM application development.

In this section we are going to see how to evaluate our `MeetingSummarizer` using **DeepEval**, a powerful open-source LLM evaluation framework.

Evaluating any LLM application has 5 phases:

1. [Defining Evaluation Criteria](#defining-evaluation-criteria)
2. [Choosing Your Metrics](#choosing-metrics)
3. [Creating Test Cases](#creating-test-cases)
4. [Running Evals](#running-evals)
5. [Creating Dataset](#creating-dataset) (_Optional_)

## Tracing

If you've added the `@observe` decorator from deepeval, you can actually just run your summarizer without any additional code and you will be able to see the traces of your application's workflow on the [Confident AI](https://www.confident-ai.com) platform.

```python
from time import sleep
summarizer = MeetingSummarizer()

with open("meeting_transcript.txt", "r") as file:
    transcript = file.read().strip()

summary, action_items = summarizer.summarize(transcript)
sleep(3) # Add this in case your traces don't appear on the platform
```

Just running the above code, I can get the following reports on the platform:

<video
  width="100%"
  autoPlay
  loop
  muted
  playsInlines
  style={{
    paddingBottom: "20px",
    height: "auto",
    maxHeight: "800px",
  }}
>
  <source
    src="https://deepeval-docs.s3.us-east-1.amazonaws.com/tutorials:summarization-agent:platform-tracing.mp4"
    type="video/mp4"
  />
</video>

You can run online evals by adding your metric collection on the platform. Click here to [learn more about tracing](https://deepeval.com/docs/evaluation-llm-tracing).

:::note
Make sure you are logged in to your [Confident AI](https://www.confident-ai.com) account to get your traces, click here to [set up](https://deepeval.com/tutorials/tutorial-setup) or run the following command in your terminal:
```bash
deepeval login
```
:::

Now let's see how we can do manual evaluations using `deepeval`.

## Defining Evaluation Criteria

Defining evaluation criteria is arguably the most important part of assessing an LLM application's performance. LLM applications are always made with a clear goal in mind, and the evaluation criteria must be defined by taking this goal into consideration. 

The summarization agent we've created processes meeting transcripts and generates a concise summary of the meeting and a list of action items. Our evaluation criteria is directly dependent on these responses which will be generated by our agent. 

- **The summaries generated must be concise and contain all important points**
- **The action items generated must be correct and cover all the key actions**

In the previous section, we've updated our summarizer to have different helper functions each for their own tasks. This makes it easier for us to evaluate each task separately.

### Choosing Metrics

From the criterion that we have defined above, both of them are use-case specific, i.e, these are custom criteria that exist only for our use case. Since both of them are custom, we will be using a custom metric twice which is 

- [G-Eval](https://deepeval.com/docs/metrics-llm-evals)

:::note 
`GEval` is a metric that uses _LLM-as-a-judge_ to evaluate LLM outputs based on **ANY** custom criteria. The `GEval` metric is the most versatile type of metric `deepeval` has to offer, and is capable of evaluating almost any use case.
:::

Here's how we will initialize our metrics:

```python
from deepeval.metrics import GEval

summary_concision = GEval(
    name="Summary Concision",
    # Write your criteria here
    criteria="Assess whether the summary is concise and focused only on the essential points of the meeting? It should avoid repetition, irrelevant details, and unnecessary elaboration.",
    threshold=0.9,
    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]
)

action_item_check = GEval(
    name="Action Item Accuracy",
    # Write your criteria here
    criteria="Are the action items accurate, complete, and clearly reflect the key tasks or follow-ups mentioned in the meeting?",
    threshold=0.9,
    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]
)
```

## Creating Test Cases 

We will now call our summarization agent and create our `LLMTestCase`s that we can use to evaluate our agent.

We will now call our summarization agent and create our test cases. Since our summarization agent returns summary and action items seperately, we will create 2 test cases for 1 `summarize()` call.

Here's how we will create our test cases:

```python
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from meeting_summarizer import MeetingSummarizer # import your summarizer here

with open("meeting_transcript.txt", "r") as file:
    transcript = file.read().strip()

summarizer = MeetingSummarizer(...)
summary, action_items = summarizer.summarize(transcript)

summary_test_case = LLMTestCase(
    input=transcript, # your full meeting transcript as a string
    actual_output=summary # provide the summary generated by your summarizer here
)

action_item_test_case = LLMTestCase(
    input=transcript, # your full meeting transcript as a string
    actual_output=str(action_items) # provide the action items generated by your summarizer here
)
```

## Running Evals

If we are trying to create a summarization agent, we might already have a folder containing transcripts that needs to be summarized, we will now loop over those transcripts and create our `LLMTestCase`s and run evaluations on our summarization agent using the previously defined metrics.


```python title="test_summarizer.py"
from deepeval.test_case import LLMTestCase, LLMTestCaseParams
from deepeval.metrics import GEval
from deepeval import evaluate
from meeting_summarizer import MeetingSummarizer # import your summarizer here

documents_path = "path/to/documents/folder"
transcripts = []
for document in os.listdir(documents_path):
    if document.endswith(".txt"):
        file_path = os.path.join(documents_path, document)
        with open(file_path, "r") as file:
            transcript = file.read().strip()
        transcripts.append(transcript)

summarizer = MeetingSummarizer()

summary_test_cases = []
action_item_test_cases = []
for transcript in transcripts:
    summary, action_items = summarizer.summarize(transcript)

    summary_test_case = LLMTestCase(
        input=transcript, # your full meeting transcript as a string
        actual_output=summary # provide the summary generated by your summarizer here
    )
    action_item_test_case = LLMTestCase(
        input=transcript, # your full meeting transcript as a string
        actual_output=str(action_items) # provide the action items generated by your summarizer here
    )
    summary_test_cases.append(summary_test_case)
    action_item_test_cases.append(action_item_test_case)

summary_concision = GEval(...) # Initialize this same as shown above
action_item_check = GEval(...) # Initialize this same as shown above

evaluate(test_cases=summary_test_cases, metrics=[summary_concision])
evaluate(test_cases=action_item_test_cases, metrics=[action_item_check])
```

You can save the above code in a test file named `test_summarizer.py` and run the following code in your terminal to evaluate your summarizer:

```bash
deepeval test run test_summarizer.py
```

You'll see your evaluation results, including scores and reasoning, printed in the console.

:::tip
It is highly recommended that you use [**Confident AI**](https://www.confident-ai.com), `deepeval`'s cloud platform that allows you to view your test results in a much more intuitive way. Here's how you can [set up Confident AI](https://deepeval.com/tutorials/tutorial-setup#setting-up-confident-ai). Or you can simply run the following code in the terminal to set it up yourself:
```bash
deepeval login
```
**It's free to get started!** _(No credit card required.)_
:::

### Evaluation Results

Here are the average results I got after running the evaluations:

| Metric                    | Score |
|---------------------------|-------|
| Summary Concision         | 0.7   |
| Action Item Accuracy      | 0.8   |

**DeepEval**'s metrics provide a reason for their evaluation of a test case, which allows you to debug your LLM application easily on why certain test cases pass or fail. Below is one of the reasons from a failed test case provided by `deepeval`'s `GEval` for the above evaluations:

For summary:


> The Actual Output effectively identifies the key points of the meeting, covering the issues with the assistant's performance, the comparison between GPT-4o and Claude 3, the proposed hybrid approach, and the discussion around confidence metrics and tone. It omits extraneous details and is significantly shorter than the Input transcript. There's minimal repetition. However, while concise, it could be *slightly* more reduced; some phrasing feels unnecessarily verbose for a summary (e.g., 'Ethan and Maya discussed... focusing on concerns').

For action items: 

> The Actual Output captures some key action items discussed in the Input, specifically Maya building the similarity metric and setting up the hybrid model test, and Ethan syncing with design. However, it misses several follow-ups, such as exploring 8-bit embedding quantization and addressing the robotic tone of the assistant via prompt tuning. While the listed actions are clear and accurate, the completeness is lacking. The action items directly correspond to tasks mentioned, but not all tasks are represented.

### Understanding Eval Results

Let's understand how the evaluation happened and why our test cases failed. `GEval` uses _LLM-as-a-judge_ to evaluate the results of the test case.

If we look at a test case and metrics we've provided for evaluation:

```python
summary_test_case = LLMTestCase(
    input=transcript, # your full meeting transcript as a string
    actual_output=summary # provide the summary generated by your summarizer here
)

action_item_test_case = LLMTestCase(
    input=transcript, # your full meeting transcript as a string
    actual_output=str(action_items) # provide the action items generated by your summarizer here
)

summary_concision = GEval(
    name="Summary Concision",
    criteria="Assess whether the summary is concise and focused only on the essential points of the meeting? It should avoid repetition, irrelevant details, and unnecessary elaboration.",
    threshold=0.9,
    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]
)

action_item_check = GEval(
    name="Action Item Accuracy",
    criteria="Are the action items accurate, complete, and clearly reflect the key tasks or follow-ups mentioned in the meeting?",
    threshold=0.9,
    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT]
)
```

We can see that we are supplying our **_transcript_** as `input`s and the **_summary_** or **_action items_** generated as the `actual_output`s in our test cases. Now `GEval` uses it's corresponding `criteria` which we've supplied, as guidelines to be followed while evaluating our test cases.

This means a new LLM (our evaluation model) is going to check our `input`s and `actual_output`s (_our evaluation parameters supplied in the metric initialization_) against the `criteria` mentioned. This is similar to how you would evaluate the responses yourself, but through an LLM. 

This new LLM assesses the quality of the `actual_output` against the `input` depending on the `criteria` mentioned. After assessing the test case, the evaluation model provides a `score` and `reason` explaining the evaluation results. If the score is below the `threshold` supplied in the metric, the test case is deemed fail, and success otherwise. (_The default threshold is 0.5 for most test cases in `deepeval`_)

These reasons explain why the test cases failed, and help us identify exactly what needs to be fixed. Click here to learn more about [how `GEval` works.](https://www.confident-ai.com/blog/g-eval-the-definitive-guide)

:::info
It is advised to use a good evaluation model for better results and reasons. Your evaluation model should be well-suited for the task it's evaluating.
Some models like `gpt-4`, `gpt-4o`, `gpt-3.5-turbo` and `claude-3-opus` are best for summarization evaluations.
:::

## Creating Dataset

During evaluation, we've seen how we loop over a folder to get all the transcripts and store them as inputs which are later used to create `LLMTestCase`s. This is not always possible when you're trying to evaluate your agent's performace each time. 

For a use case like summarization agent, the number of transcripts to summarize will keep increasing and your agent must be able to handle the new data. This is why we create evaluation datasets that contain inputs and can be used to generate summaries anytime without having to loop over a directory of transcripts.

**DeepEval**'s' [datasets](https://deepeval.com/docs/evaluation-datasets), which are simply a collection of `Golden`s, can be stored in cloud and pulled anytime with just a few lines of code. This allows you to create a dataset that can be used to create test cases by calling your LLM application and evaluate these test cases during run time.

![Evaluation Dataset](https://deepeval-docs.s3.us-east-1.amazonaws.com/docs:evaluation-dataset.png)

### Goldens in DeepEval

A dataset can only be created with a list of goldens, and it's important to know how they are different from test cases. `Golden`s represent a more flexible alternative to test cases in the `deepeval`, and **it is the preferred way to initialize a dataset using goldens**. Unlike test cases, `Golden`s:

- Don't require an `actual_output` when created
- Store expected results like `expected_output` and `expected_tools`
- Serve as templates before becoming fully-formed test cases

This means you can store these goldens in the cloud and create your own test cases during run time by pulling this dataset and generating your `actual_output`s by calling your LLM application.

### Creating Goldens

For our summarization agent, we only have 2 essential parameters required to create LLM test cases that allow us to evaluate our summarizer. These are `input` and `actual_output`, We can create a dataset that contains numerous goldens each corresponding to different meeting transcripts represented as `input`s which can later be used to create `LLMTestCase`s during runtime. Here's how you can create those goldens:

```python {2,16-18}
import os
from deepeval.dataset import Golden

documents_path = "path/to/documents/folder"
transcripts = []

for document in os.listdir(documents_path):
    if document.endswith(".txt"):
        file_path = os.path.join(documents_path, document)
        with open(file_path, "r") as file:
            transcript = file.read().strip()
        transcripts.append(transcript)

goldens = []
for transcript in transcripts:
    golden = Golden(
        input=transcript
    )
    goldens.append(golden)
```

You can view your goldens as shown below:

```python
for i, golden in enumerate(goldens):
    print(f"Golden {i}: ", golden.input[:20])
```


### Saving Dataset

We can use the above created goldens to initialize a dataset and store it in cloud. Here's how you can do that:

```python
from deepeval.dataset import EvaluationDataset

dataset = EvaluationDataset(goldens=goldens)
dataset.push(alias="MeetingSummarizer Dataset")
```
:::note
You must be logged in to your [Confident AI](https://confident-ai.com) account to manage datasets on cloud. Set up Confident AI as shown [here](https://deepeval.com/tutorials/tutorial-setup#setting-up-confident-ai) or just run the following code in your terminal to get started:
```bash
deepeval login
```
:::

These stored datasets can later be pulled and used wherever needed.

In the next section, we are going to see how to pull these datasets and run evaluations to [test and improve our summarization agent](tutorial-summarization-improvement).