---
id: tutorial-rag-qa-evaluation
title: Evaluating Your RAG Components
sidebar_label: Evaluate Retriever & Generator
---

In this tutorial, we are going to see how to evaluate our `RAGAgent` using **DeepEval**, a powerful open-source LLM evaluation framework.

All LLM applications can be evaluated in 5 steps:

1. [Defining Evaluation Criteria](#defining-evaluation-criteria)
2. [Choosing Your Metrics](#choosing-metrics)
3. [Creating Test Cases](#creating-test-cases)
4. [Running Evals](#running-evals)
5. [Creating Dataset](#creating-dataset) (_Optional_)

## Tracing

In the previous section we've added the `@observe` decorator to our components during development. If you've added this decorator to your RAG agent, you will be able to see the traces of your agent's entire workflow on the [Confident AI](https://www.confident-ai.com) platform without a single extra line of code.

```python
from time import sleep
document_paths = ["theranos_legacy.txt"]
query = "What is the NanoDrop, and what certifications does Theranos hold?"

retriever = RAGAgent(document_paths)
answer = retriever.answer(query)
sleep(3) # Add this in case your traces don't appear on the platform
```

After running the above code, I can get the following reports on the platform:

<video
  width="100%"
  autoPlay
  loop
  muted
  playsInlines
  style={{
    paddingBottom: "20px",
    height: "auto",
    maxHeight: "800px",
  }}
>
  <source
    src="https://deepeval-docs.s3.us-east-1.amazonaws.com/tutorials:rag-qa-agent:platform-tracing.mp4"
    type="video/mp4"
  />
</video>

You can run online evals by adding your metric collection on the platform. Click here to [learn more about tracing](https://deepeval.com/docs/evaluation-llm-tracing).

:::note
Make sure you are logged in to your [Confident AI](https://www.confident-ai.com) account to get your traces, click here to [set up](https://deepeval.com/tutorials/tutorial-setup) or run the following command in your terminal:
```bash
deepeval login
```
:::

## Defining Evaluation Criteria

**Retrieval-Augmented Generation (RAG)** applications are made up of two components — a retriever and a generator. Each component performs it's own tasks and is prune to errors in their own ways, which is why it is necessary to evaluate them in isolation and as a single unit too. 

For a **retriever**, the retrieved context must follow the below criterion:

- The retrieved context must be relevant
- The retrieved context must be complete and enough to answer the query
- The retrieved context must not contain any additional unnecessary information

For a **generator**, we will have to define criteria based on the use case, in our case the QA agent will respond to us in `json` format, and hence we will be using a custom metric to evaluate the following criteria:

- Make sure the "answer" property in json is correct for the query and retrieved context
- The "citations" generated must be accurate and relevant

### Choosing Metrics

For the retriever, as discussed above, the retrieved context must be relevant, complete, and precise. `deepeval`'s 3 key retriever metrics test exactly on these criterion so we will be using them.

1. [Contextual Relevancy](https://deepeval.com/docs/metrics-contextual-relevancy) — _The retrieved context must be relevant to the query_
2. [Contextual Recall](https://deepeval.com/docs/metrics-contextual-recall) — _The retrieved context should be enough to answer the query_
3. [Contextual Precision](https://deepeval.com/docs/metrics-contextual-precision) — _The retrieved context should be precise and must not include unnecessary details_

Here's how we will initialize our retriever metrics:

```python
from deepeval.metrics import (
    ContextualRelevancyMetric,
    ContextualRecallMetric,
    ContextualPrecisionMetric,
)

relevancy = ContextualRelevancyMetric()
recall = ContextualRecallMetric()
precision = ContextualPrecisionMetric()
```

For the generator, since we have defined a criteria that is specific to our use case, we will be using a custom metric twice and that is the [G-Eval](https://deepeval.com/docs/metrics-llm-evals) — a custom metric that can be used to evaluate any custom criteria. We will use the following metrics for generator evaluation:
1. [Answer Correctness](https://deepeval.com/docs/metrics-llm-evals) — To evaluate only the answer from our `json`.
2. [Citation Accuracy](https://deepeval.com/docs/metrics-llm-evals) — To evaluate the citations mentioned in the `json`.

Here's how we will initialize our generator metrics:

```python
from deepeval.metrics import GEval

answer_correctness = GEval(
    name="Answer Correctness",
    criteria="Evaluate if the actual output's 'answer' property is correct and complete from the input and retrieved context. If the answer is not correct or complete, reduce score."
    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT]
)

citation_accuracy = GEval(
    name="Citation Accuracy",
    criteria="Check if the citations in the actual output are correct and relevant based on input and retrieved context. If they're not correct, reduce score."
    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT]
)
```

## Creating Test Cases 

We will now use our RAG QA agent to answer a few user queries to generate some `LLMTestCase`s that we can use to evaluate our agent. We will create them with the queries as `input`s and the agent's responses as `actual_output`s.

Here's how we can create `LLMTestCase`s by using our QA agent:

```python
from deepeval.test_case import LLMTestCase
from qa_agent import RAGAgent

document_paths = ["theranos_legacy.txt"]
retriever = RAGAgent(document_paths)

query = "What is the NanoDrop, and what certifications does Theranos hold?"
retrieved_docs = retriever.retrieve(query)
answer = retriever.generate(query, retrieved_docs)

test_case = LLMTestCase(
    input=query,
    actual_output=answer,
    retrieval_context=retrieved_docs
)
```

## Running Evals

For evaluating a QA agent, we need a lof of question-answer pairs to use them as reference to evaluate our agent. However, generating these pairs is not an easy task and requires a lot of effort.

We can solve this problem by generating synthetic question-answer pairs using LLMs, `deepeval`'s synthesizer can help you do that with just a few lines of code.

### Synthesizer

You can create `Golden`s of question-answer pairs by using `deepeval`'s [`Synthesizer`](https://deepeval.com/docs/synthesizer-introduction). Here's how you can use the synthesizer:

```python
from deepeval.synthesizer import Synthesizer

synthesizer = Synthesizer()

goldens = synthesizer.generate_goldens_from_docs(
    # Provide the path to your documents
    document_paths=['theranos_legacy.txt', 'theranos_legacy.docx', 'theranos_legacy.pdf']
)
```

This above code snippet returns a list of `Golden`s, that contain `input` and `expected_output`. We can use these goldens to create `LLMTestCase`s by calling our RAG QA agent.

In this tutorial, we'll be doing retriever evaluation and genrator evaluation seperately. However, it advised to also do combined evaluations to co-optimise your RAG QA agent as a whole.

### Retriever Evaluation

Now we can use the goldens we just created to evaluate the retriever. Here's how we can evaluate our retriever using the _relevancy, recall and precision_ metrics that we've defined above:

```python title="test_retriever.py"
from deepeval.test_case import LLMTestCase
from deepeval.metrics import (
    ContextualRelevancyMetric,
    ContextualRecallMetric,
    ContextualPrecisionMetric,
)
from qa_agent import RAGAgent

# Initialize metrics
relevancy = ContextualRelevancyMetric()
recall = ContextualRecallMetric()
precision = ContextualPrecisionMetric()

metrics = [relevancy, recall, precision]

# Evaluate for each golden
document_path = ["theranos_legacy.txt"]
retriever = RAGAgent(document_path)

retriever_test_cases = []
for golden in goldens:
    retrieved_docs = retriever.retrieve(golden.input)
    test_case = LLMTestCase(
        input=golden.input,
        actual_output=golden.expected_output,
        expected_output=golden.expected_output,
        retrieval_context=retrieved_docs
    )
    retriever_test_cases.append(test_case)

evaluate(retriever_test_cases, metrics)
```

You can save the above code in a test file named `test_retriever.py` and run the following code in your terminal to evaluate your retriever:

```bash
deepeval test run test_retriever.py
```

You'll see your evaluation results, including scores and reasoning, printed in the console.

:::tip
It is highly recommended that you use [**Confident AI**](https://www.confident-ai.com), `deepeval`'s cloud platform that allows you to view your test results in a much more intuitive way. Here's how you can [set up Confident AI](https://deepeval.com/tutorials/tutorial-setup#setting-up-confident-ai). Or you can simply run the following code in the terminal to set it up yourself:
```bash
deepeval login
```
**It's free to get started!** _(No credit card required.)_
:::

After running this evaluation, I got the following average scores:

| Metric               | Score |
|----------------------|-------|
| Contextual Relevancy | 0.6   |
| Contextual Recall    | 0.7   |
| Contextual Precision | 0.6   |

These results can be improved by tuning the hyperparameters of your retriever, we'll see how we can improve our retriver in the improvement section of this tutorial. Now let's move on to generator evaluation.

### Generator Evaluation

We can use the exact same goldens to evaluate our generator by using the generator metrics we've defined above. Here's how we can evaluate the generator:

```python title="test_generator.py"
from deepeval.metrics import GEval
from deepeval.test_case import LLMTestCase, LLMTestCaseParams

answer_correctness = GEval(
    name="Answer Correctness",
    criteria="Evaluate if the actual output's 'answer' property is correct and complete from the input and retrieved context. If the answer is not correct or complete, reduce score."
    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT]
)

citation_accuracy = GEval(
    name="Citation Accuracy",
    criteria="Check if the citations in the actual output are correct and relevant based on input and retrieved context. If they're not correct, reduce score."
    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.RETRIEVAL_CONTEXT]
)

metrics = [answer_correctness, citation_accuracy]

generator_test_cases = []
for golden in goldens:
    retrieved_docs = retriever.retrieve(golden.input)
    generated_answer = retriever.generate(golden.input, retrieved_docs)
    test_case = LLMTestCase(
        input=golden.input,
        actual_output=str(generated_answer),
        expected_output=golden.expected_output,
        retrieval_context=retrieved_docs
    )
    dataset.add_test_case(test_case)
    generator_test_cases.append(test_case)

evaluate(generator_test_cases, metrics)
```

You can save the above code in a test file named `test_generator.py` and run the following code in your terminal to evaluate your generator:

```bash
deepeval test run test_generator.py
```

After running this evaluation, I got the following average scores:

| Metric               | Score |
|----------------------|-------|
| Answer Correctness   | 0.5   |
| Citation Accuracy    | 0.5   |

Most of `deepeval`'s metrics provide a reason along with a score which let's us know why certain test cases fail. From all the reasons, these were the observations made:

For answer correctness:

> The answers were too short and informal, they can benefit from being a bit more professional and informative.

For citation accuracy:

> There are many irrelevant and hallucinating citations which could mislead a user about the product.

## Creating Dataset

During evaluation, we've seen how we can create synthetic `Golden`s of question-answer pairs that can be used to evaluate our RAG agent. Creating these `Golden`s everytime you want to evaluate your RAG agent is a computationally expensive task, hence we need to store these `Golden`s somewhere.

**DeepEval**'s' [datasets](https://deepeval.com/docs/evaluation-datasets), which are simply a collection of `Golden`s, can be stored in cloud and pulled anytime with just a few lines of code. This allows you to create a dataset that can be used to create `LLMTestCase`s by calling your RAG agent and evaluate these test cases during run time.

![Evaluation Dataset](https://deepeval-docs.s3.us-east-1.amazonaws.com/docs:evaluation-dataset.png)

We will use the goldens generated by using `deepeval`'s [`Synthesizer`](https://deepeval.com/docs/synthesizer-introduction) as shown before to create an [`EvaluationDataset`](https://deepeval.com/docs/evaluation-datasets#create-a-dataset).

```python
from deepeval.synthesizer import Synthesizer

synthesizer = Synthesizer()

goldens = synthesizer.generate_goldens_from_docs(
    # Provide the path to your documents
    document_paths=['theranos_legacy.txt', 'theranos_legacy.docx', 'theranos_legacy.pdf']
)
```

You can view your goldens as shown below:

```python
for i, golden in enumerate(goldens):
    print(f"Golden {i}: ", golden.input[:20])
```

### Saving Dataset

Now that you have your goldens, you can use these goldens to create an `EvaluationDataset`. Here's how you can create a dataset:

```python
from deepeval.dataset import EvaluationDataset

dataset = EvaluationDataset(goldens=goldens)
dataset.push(alias="QA Agent Dataset")
```

Here's what the dataset looks like on the Confident AI platform:

![Dataset on Confident AI Platform](https://deepeval-docs.s3.us-east-1.amazonaws.com/tutorials:qa-agent:dataset-platform.png)

You can edit, finalize and do much more to this dataset on the [Confident AI](https://www.confident-ai.com) platform.

:::note
You must be logged in to your [Confident AI](https://confident-ai.com) account to manage datasets on cloud. Set up Confident AI as shown [here](https://deepeval.com/tutorials/tutorial-setup#setting-up-confident-ai) or just run the following code in your terminal to get started:
```bash
deepeval login
```
:::

These stored datasets can later be pulled and used wherever needed.

In the next section we will see how to pull this dataset and iterate over multiple hyperparameters for our RAG agent and evaluate it to see [how to improve our RAG agent](/tutorials/rag-qa-agent/tutorial-rag-qa-improvement).