import os
import asyncio
import sys
from typing import List, Dict

# è·å–é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„å¹¶æ·»åŠ  DeepEval æºç è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DEEPEVAL_SOURCE_PATH = os.path.join(PROJECT_ROOT, 'deepeval_source')

if DEEPEVAL_SOURCE_PATH not in sys.path:
    sys.path.insert(0, DEEPEVAL_SOURCE_PATH)
    print(f"ğŸ”§ å·²æ·»åŠ æœ¬åœ° DeepEval æºç è·¯å¾„: {DEEPEVAL_SOURCE_PATH}")

# éªŒè¯å¯¼å…¥è·¯å¾„
try:
    import deepeval
    print(f"âœ… æˆåŠŸå¯¼å…¥ DeepEvalï¼Œè·¯å¾„: {deepeval.__file__}")
    if 'deepeval_source' in deepeval.__file__:
        print("ğŸ¯ ä½¿ç”¨çš„æ˜¯æœ¬åœ° DeepEval æºç ")
    else:
        print("âš ï¸  è­¦å‘Šï¼šä½¿ç”¨çš„æ˜¯ç³»ç»Ÿå®‰è£…çš„ DeepEval åŒ…")
except ImportError as e:
    print(f"âŒ DeepEval å¯¼å…¥å¤±è´¥: {e}")
    raise

# å¯¼å…¥æ‰€éœ€çš„æ¨¡å—
from deepeval.synthesizer import Synthesizer
from deepeval.synthesizer.config import StylingConfig
from deepeval.dataset import EvaluationDataset
from utils.logger import get_logger
from config import API_CONFIG

# è·å–æ—¥å¿—è®°å½•å™¨
logger = get_logger('deepeval_generator', 'business')

class DeepEvalDatasetGenerator:
    """ä½¿ç”¨DeepEvalç”Ÿæˆæ•°æ®é›†çš„å·¥å…·ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–DeepEvalæ•°æ®é›†ç”Ÿæˆå™¨"""
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ["OPENAI_API_KEY"] = API_CONFIG['openai_api_key']
        os.environ["OPENAI_BASE_URL"] = API_CONFIG['openai_base_url']
        
        # åˆå§‹åŒ–Synthesizer
        self.synthesizer = Synthesizer(
            model=API_CONFIG['openai_model'],
            async_mode=True,
            max_concurrent=10
        )
        
        logger.info(f"DeepEvalæ•°æ®é›†ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {API_CONFIG['openai_model']}")
    
    def _prepare_contexts_with_instruction(self, contexts: List[str]) -> List[List[str]]:
        """ä¸ºä¸Šä¸‹æ–‡æ·»åŠ æ¨ç†æŒ‡ç¤º"""
        instruction = "è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯è¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆä¸­æ–‡é—®é¢˜å’Œç­”æ¡ˆã€‚è¦æ±‚ï¼š1. é—®é¢˜å¿…é¡»ç”¨ä¸­æ–‡æé—®ï¼›2. ç­”æ¡ˆå¿…é¡»ç”¨ä¸­æ–‡å›ç­”ï¼›3. å†…å®¹è¦ç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯ï¼›4. ä¸è¦ç”Ÿæˆä»»ä½•è‹±æ–‡å†…å®¹ã€‚"
        contexts_with_instruction = [instruction] + contexts
        return [contexts_with_instruction]
    
    def _create_styling_config(self, scenario: str = "educational") -> StylingConfig:
        """åˆ›å»ºç”Ÿæˆé£æ ¼é…ç½®"""
        return StylingConfig(
            scenario=scenario,  # æ•™è‚²åœºæ™¯
            task="ç”Ÿæˆä¸­æ–‡é—®ç­”å¯¹ã€‚ä¸¥æ ¼è¦æ±‚ï¼š1. é—®é¢˜å¿…é¡»ç”¨ä¸­æ–‡æé—®ï¼›2. ç­”æ¡ˆå¿…é¡»ç”¨ä¸­æ–‡å›ç­”ï¼›3. å†…å®¹è¦ç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯ï¼›4. ä¸è¦ç”Ÿæˆä»»ä½•è‹±æ–‡å†…å®¹ï¼›5. æ‰€æœ‰æ–‡æœ¬å¿…é¡»æ˜¯ä¸­æ–‡ã€‚",  # é—®é¢˜ç”Ÿæˆä»»åŠ¡
            input_format="ä¸­æ–‡é—®é¢˜ï¼Œå¿…é¡»ä»¥ä¸­æ–‡å¼€å¤´ï¼Œä¸èƒ½åŒ…å«è‹±æ–‡",  # è¾“å…¥æ ¼å¼ä¸ºä¸­æ–‡
            expected_output_format="ä¸­æ–‡ç­”æ¡ˆï¼Œå¿…é¡»ç”¨ä¸­æ–‡å›ç­”ï¼Œä¸èƒ½åŒ…å«è‹±æ–‡"  # æœŸæœ›è¾“å‡ºæ ¼å¼ä¸ºä¸­æ–‡
        )
    

    
    async def generate_from_contexts(
        self, 
        contexts: List[str], 
        num_questions: int,
        scenario: str = "educational"
    ) -> List[Dict]:
        """
        ä»ä¸Šä¸‹æ–‡ç”Ÿæˆé—®ç­”å¯¹ï¼Œä¸€æ¬¡æ€§å®Œæˆæ‰€æœ‰ç”Ÿæˆ
        
        Args:
            contexts: ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ—è¡¨
            num_questions: éœ€è¦ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            scenario: ç”Ÿæˆåœºæ™¯ (educational, conversational, technical)
            
        Returns:
            ç”Ÿæˆçš„é—®ç­”å¯¹åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨DeepEvalç”Ÿæˆæ•°æ®é›†: ä¸Šä¸‹æ–‡æ•°é‡={len(contexts)}, é—®é¢˜æ•°é‡={num_questions}")
            
            # å‡†å¤‡ä¸Šä¸‹æ–‡
            contexts_with_instruction = self._prepare_contexts_with_instruction(contexts)
            
            # åˆ›å»ºé£æ ¼é…ç½®
            styling_config = self._create_styling_config(scenario)
            # è¿›ä¸€æ­¥å¼ºåŒ–ä¸­æ–‡ç”Ÿæˆè¦æ±‚
            styling_config.task = "åŸºäºä¸Šä¸‹æ–‡å†…å®¹ç”Ÿæˆä¸­æ–‡é—®ç­”å¯¹ã€‚ä¸¥æ ¼è¦æ±‚ï¼š1. é—®é¢˜å¿…é¡»ç”¨ä¸­æ–‡æé—®ï¼›2. ç­”æ¡ˆå¿…é¡»ç”¨ä¸­æ–‡å›ç­”ï¼›3. å†…å®¹è¦ç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯ï¼›4. ä¸è¦ç”Ÿæˆä»»ä½•è‹±æ–‡å†…å®¹ï¼›5. æ‰€æœ‰æ–‡æœ¬å¿…é¡»æ˜¯ä¸­æ–‡ï¼›6. ç¦æ­¢ä½¿ç”¨è‹±æ–‡å•è¯æˆ–çŸ­è¯­ã€‚"
            
            # è®¡ç®—æ¯ä¸ªä¸Šä¸‹æ–‡ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            max_goldens_per_context = max(1, num_questions // len(contexts_with_instruction))
            
            logger.info(f"ä¸€æ¬¡æ€§ç”Ÿæˆ {num_questions} ä¸ªé—®ç­”å¯¹ï¼Œæ¯ä¸ªä¸Šä¸‹æ–‡ç”Ÿæˆ {max_goldens_per_context} ä¸ª")
            
            # è®¾ç½®é£æ ¼é…ç½®
            self.synthesizer.styling_config = styling_config
            
            # ä½¿ç”¨DeepEvalä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰é—®ç­”å¯¹
            dataset: EvaluationDataset = await self.synthesizer.a_generate_goldens_from_contexts(
                contexts=contexts_with_instruction,
                include_expected_output=True,
                max_goldens_per_context=max_goldens_per_context
            )
            
            # è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
            qa_items = []
            for golden in dataset:
                qa_item = {
                    'question': golden.input,
                    'expected_output': golden.expected_output,
                    'context': contexts,
                    'context_length': sum(len(x) for x in contexts)
                }
                qa_items.append(qa_item)
            
            # å»é‡å¤„ç†
            final_items = []
            seen_questions = set()
            
            for item in qa_items:
                if item['question'] not in seen_questions:
                    final_items.append(item)
                    seen_questions.add(item['question'])
                else:
                    logger.warning(f"å‘ç°é‡å¤é—®é¢˜ï¼Œè·³è¿‡: {item['question'][:50]}...")
            
            # åªå–éœ€è¦çš„æ•°é‡
            final_items = final_items[:num_questions]
            
            logger.info(f"DeepEvalç”Ÿæˆå®Œæˆï¼Œæ€»å…±ç”Ÿæˆäº† {len(final_items)} ä¸ªé—®ç­”å¯¹")
            
            return final_items
            
        except Exception as e:
            logger.error(f"DeepEvalç”Ÿæˆå¤±è´¥: {str(e)}")
            raise
    
    async def generate_from_documents(
        self, 
        document_paths: List[str], 
        num_questions: int,
        scenario: str = "educational"
    ) -> List[Dict]:
        """
        ä»æ–‡æ¡£ç”Ÿæˆé—®ç­”å¯¹
        
        Args:
            document_paths: æ–‡æ¡£è·¯å¾„åˆ—è¡¨
            num_questions: éœ€è¦ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            scenario: ç”Ÿæˆåœºæ™¯
            
        Returns:
            ç”Ÿæˆçš„é—®ç­”å¯¹åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨DeepEvalä»æ–‡æ¡£ç”Ÿæˆæ•°æ®é›†: æ–‡æ¡£æ•°é‡={len(document_paths)}, é—®é¢˜æ•°é‡={num_questions}")
            
            # è®¡ç®—æ¯ä¸ªæ–‡æ¡£ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            max_goldens_per_context = max(1, num_questions // len(document_paths))
            
            # åˆ›å»ºé£æ ¼é…ç½®å¹¶è®¾ç½®åˆ° synthesizer
            styling_config = self._create_styling_config(scenario)
            # è¿›ä¸€æ­¥å¼ºåŒ–ä¸­æ–‡ç”Ÿæˆè¦æ±‚
            styling_config.task = "åŸºäºæ–‡æ¡£å†…å®¹ç”Ÿæˆä¸­æ–‡é—®ç­”å¯¹ã€‚ä¸¥æ ¼è¦æ±‚ï¼š1. é—®é¢˜å¿…é¡»ç”¨ä¸­æ–‡æé—®ï¼›2. ç­”æ¡ˆå¿…é¡»ç”¨ä¸­æ–‡å›ç­”ï¼›3. å†…å®¹è¦ç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯ï¼›4. ä¸è¦ç”Ÿæˆä»»ä½•è‹±æ–‡å†…å®¹ï¼›5. æ‰€æœ‰æ–‡æœ¬å¿…é¡»æ˜¯ä¸­æ–‡ï¼›6. ç¦æ­¢ä½¿ç”¨è‹±æ–‡å•è¯æˆ–çŸ­è¯­ã€‚"
            self.synthesizer.styling_config = styling_config
            
            # ä½¿ç”¨DeepEvalç”Ÿæˆæ•°æ®é›†
            dataset: EvaluationDataset = await self.synthesizer.a_generate_goldens_from_docs(
                document_paths=document_paths,
                include_expected_output=True,
                max_goldens_per_context=max_goldens_per_context
            )
            
            # è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
            qa_items = []
            for golden in dataset:
                # è·å–çœŸå®çš„æ–‡æ¡£ç‰‡æ®µå†…å®¹
                context_content = []
                if hasattr(golden, 'context') and golden.context:
                    # å¦‚æœ Golden å¯¹è±¡æœ‰ context å±æ€§ï¼Œç›´æ¥ä½¿ç”¨
                    if isinstance(golden.context, list):
                        context_content = golden.context
                    else:
                        context_content = [str(golden.context)]
                elif hasattr(golden, 'source_file'):
                    # å¦‚æœæ²¡æœ‰ contextï¼Œè‡³å°‘è®°å½•æ¥æºæ–‡ä»¶
                    context_content = [f"æ–‡æ¡£: {golden.source_file}"]
                
                qa_item = {
                    'question': golden.input,
                    'expected_output': golden.expected_output,
                    'context': context_content,
                    'context_length': sum(len(str(x)) for x in context_content)
                }
                qa_items.append(qa_item)
            
            logger.info(f"DeepEvalæ–‡æ¡£ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(qa_items)} ä¸ªé—®ç­”å¯¹")
            return qa_items
            
        except Exception as e:
            logger.error(f"DeepEvalæ–‡æ¡£ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise
    
    async def generate_from_scratch(
        self, 
        num_questions: int,
        scenario: str = "educational",
        topic: str = None
    ) -> List[Dict]:
        """
        ä»é›¶å¼€å§‹ç”Ÿæˆé—®ç­”å¯¹
        
        Args:
            num_questions: éœ€è¦ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            scenario: ç”Ÿæˆåœºæ™¯
            topic: ä¸»é¢˜ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç”Ÿæˆçš„é—®ç­”å¯¹åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨DeepEvalä»é›¶ç”Ÿæˆæ•°æ®é›†: é—®é¢˜æ•°é‡={num_questions}, ä¸»é¢˜={topic}")
            
            # åˆ›å»ºé£æ ¼é…ç½®å¹¶è®¾ç½®åˆ° synthesizer
            styling_config = self._create_styling_config(scenario)
            
            # å¦‚æœæœ‰ä¸»é¢˜ä¿¡æ¯ï¼Œä¿®æ”¹ä»»åŠ¡æè¿°
            if topic:
                styling_config.task = f"åŸºäºä¸»é¢˜'{topic}'ç”Ÿæˆä¸­æ–‡é—®é¢˜å’Œç­”æ¡ˆã€‚ä¸¥æ ¼è¦æ±‚ï¼š1. é—®é¢˜å¿…é¡»ç”¨ä¸­æ–‡æé—®ï¼›2. ç­”æ¡ˆå¿…é¡»ç”¨ä¸­æ–‡å›ç­”ï¼›3. å†…å®¹è¦ç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯ï¼›4. ä¸è¦ç”Ÿæˆä»»ä½•è‹±æ–‡å†…å®¹ï¼›5. æ‰€æœ‰æ–‡æœ¬å¿…é¡»æ˜¯ä¸­æ–‡ï¼›6. ç¦æ­¢ä½¿ç”¨è‹±æ–‡å•è¯æˆ–çŸ­è¯­ï¼›7. é—®é¢˜å¿…é¡»ä»¥ä¸­æ–‡å¼€å¤´ï¼Œä¸èƒ½ä»¥è‹±æ–‡å¼€å¤´ï¼›8. ç»å¯¹ä¸å…è®¸ç”Ÿæˆè‹±æ–‡é—®é¢˜ï¼›9. é—®é¢˜å¿…é¡»ä»¥'ä»€ä¹ˆæ˜¯'ã€'å¦‚ä½•'ã€'ä¸ºä»€ä¹ˆ'ã€'è¯·è§£é‡Š'ç­‰ä¸­æ–‡è¯æ±‡å¼€å¤´ï¼›10. ç¦æ­¢ä½¿ç”¨ä»»ä½•è‹±æ–‡å•è¯ï¼ŒåŒ…æ‹¬æŠ€æœ¯æœ¯è¯­ä¹Ÿè¦ç”¨ä¸­æ–‡è¡¨è¾¾ã€‚"
                styling_config.scenario = f"å…³äº{topic}çš„ä¸­æ–‡æ•™è‚²é—®ç­”"
                styling_config.input_format = "ä¸­æ–‡é—®é¢˜ï¼Œå¿…é¡»ä»¥ä¸­æ–‡å¼€å¤´ï¼Œä¸èƒ½åŒ…å«è‹±æ–‡"
                styling_config.expected_output_format = "ä¸­æ–‡ç­”æ¡ˆï¼Œå¿…é¡»ç”¨ä¸­æ–‡å›ç­”ï¼Œä¸èƒ½åŒ…å«è‹±æ–‡"
            
            self.synthesizer.styling_config = styling_config
            
            # ä½¿ç”¨DeepEvalç”Ÿæˆæ•°æ®é›†
            dataset: EvaluationDataset = await self.synthesizer.a_generate_goldens_from_scratch(
                num_goldens=num_questions
            )
            
            # è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
            qa_items = []
            for golden in dataset:
                # å¤„ç† expected_output ä¸º None çš„æƒ…å†µ
                expected_output = golden.expected_output if golden.expected_output is not None else "æš‚æ— æ ‡å‡†ç­”æ¡ˆ"
                
                qa_item = {
                    'question': golden.input,
                    'expected_output': expected_output,
                    'context': [f"ä¸»é¢˜: {topic}"] if topic else [],
                    'context_length': len(topic) if topic else 0
                }
                qa_items.append(qa_item)
            
            logger.info(f"DeepEvalä»é›¶ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(qa_items)} ä¸ªä¸­æ–‡é—®ç­”å¯¹")
            

            
            return qa_items
            
        except Exception as e:
            logger.error(f"DeepEvalä»é›¶ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise
    
    async def generate_from_goldens(
        self, 
        goldens: List, 
        num_questions: int,
        scenario: str = "educational"
    ) -> List[Dict]:
        """
        ä»ç°æœ‰çš„Goldenæ•°æ®é›†ç”Ÿæˆæ–°çš„é—®ç­”å¯¹ï¼Œç”¨äºæ•°æ®é›†æ‰©å†™
        
        Args:
            goldens: ç°æœ‰çš„Goldenå¯¹è±¡åˆ—è¡¨
            num_questions: éœ€è¦ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            scenario: ç”Ÿæˆåœºæ™¯ (educational, conversational, technical)
            
        Returns:
            List[Dict]: ç”Ÿæˆçš„é—®ç­”å¯¹åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹ä»Goldenæ•°æ®é›†ç”Ÿæˆæ‰©å†™æ•°æ®: åŸå§‹æ•°æ®={len(goldens)}, ç›®æ ‡æ•°é‡={num_questions}")
            

            
            # åˆ›å»ºé£æ ¼é…ç½®
            styling_config = self._create_styling_config(scenario)
            styling_config.task = "åŸºäºç°æœ‰æ•°æ®ç”Ÿæˆä¸­æ–‡é—®ç­”å¯¹ã€‚ä¸¥æ ¼è¦æ±‚ï¼š1. é—®é¢˜å¿…é¡»ç”¨ä¸­æ–‡æé—®ï¼›2. ç­”æ¡ˆå¿…é¡»ç”¨ä¸­æ–‡å›ç­”ï¼›3. å†…å®¹è¦ç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯ï¼›4. ä¸è¦ç”Ÿæˆä»»ä½•è‹±æ–‡å†…å®¹ï¼›5. æ‰€æœ‰æ–‡æœ¬å¿…é¡»æ˜¯ä¸­æ–‡ï¼›6. ç¦æ­¢ä½¿ç”¨è‹±æ–‡å•è¯æˆ–çŸ­è¯­ã€‚"
            self.synthesizer.styling_config = styling_config
            
            # è®¡ç®—æ¯ä¸ªGoldenç”Ÿæˆå¤šå°‘ä¸ªæ–°Golden
            max_goldens_per_golden = max(1, num_questions // len(goldens))
            
            # ä½¿ç”¨DeepEvalçš„generate_goldens_from_goldensæ–¹æ³•
            new_goldens = await self.synthesizer.a_generate_goldens_from_goldens(
                goldens=goldens,
                max_goldens_per_golden=max_goldens_per_golden,
                include_expected_output=True
            )
            
            # è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
            qa_items = []
            for golden in new_goldens[:num_questions]:  # é™åˆ¶æ•°é‡
                qa_item = {
                    'question': golden.input,
                    'expected_output': golden.expected_output if golden.expected_output else "æš‚æ— æ ‡å‡†ç­”æ¡ˆ",
                    'context': golden.context if golden.context else [],
                    'context_length': len(str(golden.context)) if golden.context else 0
                }
                qa_items.append(qa_item)
            
            logger.info(f"DeepEvalä»Goldenæ‰©å†™å®Œæˆï¼Œå…±ç”Ÿæˆ {len(qa_items)} ä¸ªé—®ç­”å¯¹")
            
            return qa_items
            
        except Exception as e:
            logger.error(f"DeepEvalä»Goldenæ‰©å†™å¤±è´¥: {str(e)}")
            raise
    
    def save_dataset(self, qa_items: List[Dict], file_path: str, file_type: str = 'csv'):
        """
        ä¿å­˜æ•°æ®é›†åˆ°æ–‡ä»¶
        
        Args:
            qa_items: é—®ç­”å¯¹åˆ—è¡¨
            file_path: æ–‡ä»¶è·¯å¾„
            file_type: æ–‡ä»¶ç±»å‹ (csv, json, jsonl)
        """
        try:
            # åˆ›å»ºä¸´æ—¶æ•°æ®é›†ç”¨äºä¿å­˜
            temp_dataset = []
            for item in qa_items:
                # ç¡®ä¿ä½¿ç”¨æºç è·¯å¾„å¯¼å…¥
                from deepeval.dataset import Golden
                golden = Golden(
                    input=item['question'],
                    expected_output=item['expected_output']
                )
                temp_dataset.append(golden)
            
            # ä½¿ç”¨DeepEvalçš„ä¿å­˜åŠŸèƒ½
            self.synthesizer.synthetic_goldens = temp_dataset
            saved_path = self.synthesizer.save_as(
                file_type=file_type,
                directory=os.path.dirname(file_path),
                file_name=os.path.basename(file_path).split('.')[0],
                quiet=True
            )
            
            logger.info(f"æ•°æ®é›†å·²ä¿å­˜åˆ°: {saved_path}")
            return saved_path
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®é›†å¤±è´¥: {str(e)}")
            raise

# å…¨å±€å®ä¾‹
deepeval_generator = DeepEvalDatasetGenerator() 