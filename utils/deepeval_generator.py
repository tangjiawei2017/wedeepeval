import os
import asyncio
import sys
from typing import List, Dict, Optional, Callable

# è·å–é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DEEPEVAL_SOURCE_PATH = os.path.join(PROJECT_ROOT, 'deepeval_source')

# æ·»åŠ æœ¬åœ° DeepEval æºç è·¯å¾„åˆ° Python è·¯å¾„çš„æœ€å‰é¢
if DEEPEVAL_SOURCE_PATH not in sys.path:
    sys.path.insert(0, DEEPEVAL_SOURCE_PATH)
    print(f"ğŸ”§ å·²æ·»åŠ æœ¬åœ° DeepEval æºç è·¯å¾„: {DEEPEVAL_SOURCE_PATH}")

# éªŒè¯å¯¼å…¥è·¯å¾„
try:
    import deepeval
    print(f"âœ… æˆåŠŸå¯¼å…¥ DeepEvalï¼Œè·¯å¾„: {deepeval.__file__}")
    if 'deepeval_source' in deepeval.__file__:
        print("ğŸ¯ ä½¿ç”¨çš„æ˜¯æœ¬åœ° DeepEval æºç ")
    else:
        print("âš ï¸  è­¦å‘Šï¼šä½¿ç”¨çš„æ˜¯ç³»ç»Ÿå®‰è£…çš„ DeepEval åŒ…")
except ImportError as e:
    print(f"âŒ DeepEval å¯¼å…¥å¤±è´¥: {e}")
    raise

from deepeval.synthesizer import Synthesizer
from deepeval.synthesizer.config import StylingConfig
from deepeval.dataset import EvaluationDataset
from utils.logger import get_logger
from config import API_CONFIG, GENERATION_CONFIG

# è·å–æ—¥å¿—è®°å½•å™¨
logger = get_logger('deepeval_generator')

class DeepEvalDatasetGenerator:
    """ä½¿ç”¨DeepEvalç”Ÿæˆæ•°æ®é›†çš„å·¥å…·ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–DeepEvalæ•°æ®é›†ç”Ÿæˆå™¨"""
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ["OPENAI_API_KEY"] = API_CONFIG['openai_api_key']
        os.environ["OPENAI_BASE_URL"] = API_CONFIG['openai_base_url']
        
        # åˆå§‹åŒ–Synthesizer
        self.synthesizer = Synthesizer(
            model=API_CONFIG['openai_model'],
            async_mode=True,
            max_concurrent=10
        )
        
        logger.info(f"DeepEvalæ•°æ®é›†ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {API_CONFIG['openai_model']}")
    
    def _prepare_contexts_with_instruction(self, contexts: List[str]) -> List[List[str]]:
        """ä¸ºä¸Šä¸‹æ–‡æ·»åŠ æ¨ç†æŒ‡ç¤º"""
        instruction = "è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯è¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆä¸­æ–‡é—®é¢˜å’Œç­”æ¡ˆï¼š"
        contexts_with_instruction = [instruction] + contexts
        return [contexts_with_instruction]
    
    def _create_styling_config(self, scenario: str = "educational") -> StylingConfig:
        """åˆ›å»ºç”Ÿæˆé£æ ¼é…ç½®"""
        return StylingConfig(
            scenario=scenario,  # æ•™è‚²åœºæ™¯
            task="question_generation",  # é—®é¢˜ç”Ÿæˆä»»åŠ¡
            input_format="Chinese",  # è¾“å…¥æ ¼å¼ä¸ºä¸­æ–‡
            expected_output_format="Chinese"  # æœŸæœ›è¾“å‡ºæ ¼å¼ä¸ºä¸­æ–‡
        )
    
    def _calculate_batch_strategy(self, num_questions: int) -> Dict:
        """è®¡ç®—åˆ†æ‰¹ç”Ÿæˆç­–ç•¥"""
        if num_questions <= GENERATION_CONFIG['single_batch_threshold']:
            # å°æ•°é‡ï¼šä¸€æ¬¡æ€§ç”Ÿæˆ
            return {
                'strategy': 'single_batch',
                'batch_size': min(num_questions, GENERATION_CONFIG['max_single_batch_size']),
                'num_batches': 1
            }
        else:
            # å¤§æ•°é‡ï¼šåˆ†æ‰¹ç”Ÿæˆ
            batch_size = GENERATION_CONFIG['batch_size']
            num_batches = (num_questions + batch_size - 1) // batch_size
            return {
                'strategy': 'multi_batch',
                'batch_size': batch_size,
                'num_batches': num_batches
            }
    
    async def generate_from_contexts(
        self, 
        contexts: List[str], 
        num_questions: int,
        scenario: str = "educational",
        progress_callback: Optional[Callable[[int, int, str], None]] = None
    ) -> List[Dict]:
        """
        ä»ä¸Šä¸‹æ–‡ç”Ÿæˆé—®ç­”å¯¹ï¼Œæ”¯æŒåˆ†æ‰¹ç”Ÿæˆå’Œè¿›åº¦è·Ÿè¸ª
        
        Args:
            contexts: ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ—è¡¨
            num_questions: éœ€è¦ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            scenario: ç”Ÿæˆåœºæ™¯ (educational, conversational, technical)
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (completed, total, status)
            
        Returns:
            ç”Ÿæˆçš„é—®ç­”å¯¹åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨DeepEvalç”Ÿæˆæ•°æ®é›†: ä¸Šä¸‹æ–‡æ•°é‡={len(contexts)}, é—®é¢˜æ•°é‡={num_questions}")
            
            # è®¡ç®—åˆ†æ‰¹ç­–ç•¥
            strategy = self._calculate_batch_strategy(num_questions)
            logger.info(f"ä½¿ç”¨ç”Ÿæˆç­–ç•¥: {strategy['strategy']}, æ‰¹æ¬¡å¤§å°: {strategy['batch_size']}, æ€»æ‰¹æ¬¡æ•°: {strategy['num_batches']}")
            
            # å‡†å¤‡ä¸Šä¸‹æ–‡
            contexts_with_instruction = self._prepare_contexts_with_instruction(contexts)
            
            # åˆ›å»ºé£æ ¼é…ç½®
            styling_config = self._create_styling_config(scenario)
            
            all_qa_items = []
            completed_count = 0
            
            # åˆ†æ‰¹ç”Ÿæˆ
            for batch_num in range(strategy['num_batches']):
                batch_start = batch_num * strategy['batch_size']
                batch_end = min(batch_start + strategy['batch_size'], num_questions)
                current_batch_size = batch_end - batch_start
                
                logger.info(f"ç”Ÿæˆç¬¬ {batch_num + 1}/{strategy['num_batches']} æ‰¹ï¼Œæ•°é‡: {current_batch_size}")
                
                # æ›´æ–°è¿›åº¦
                if progress_callback:
                    progress_callback(completed_count, num_questions, f"æ­£åœ¨ç”Ÿæˆç¬¬ {batch_num + 1} æ‰¹...")
                
                # è®¡ç®—å½“å‰æ‰¹æ¬¡æ¯ä¸ªä¸Šä¸‹æ–‡ç”Ÿæˆçš„é—®é¢˜æ•°é‡
                max_goldens_per_context = max(1, current_batch_size // len(contexts_with_instruction))
                
                try:
                    # ä½¿ç”¨DeepEvalç”Ÿæˆå½“å‰æ‰¹æ¬¡
                    dataset: EvaluationDataset = await self.synthesizer.a_generate_goldens_from_contexts(
                        contexts=contexts_with_instruction,
                        include_expected_output=True,
                        max_goldens_per_context=max_goldens_per_context
                    )
                    
                    # è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
                    batch_items = []
                    for golden in dataset:
                        qa_item = {
                            'question': golden.input,
                            'expected_output': golden.expected_output,
                            'context': contexts,
                            'context_length': sum(len(x) for x in contexts)
                        }
                        batch_items.append(qa_item)
                    
                    # å»é‡å¤„ç†
                    new_items = []
                    existing_questions = set(item['question'] for item in all_qa_items)
                    
                    for item in batch_items:
                        if item['question'] not in existing_questions:
                            new_items.append(item)
                            existing_questions.add(item['question'])
                        else:
                            logger.warning(f"å‘ç°é‡å¤é—®é¢˜ï¼Œè·³è¿‡: {item['question'][:50]}...")
                    
                    # æ·»åŠ åˆ°æ€»åˆ—è¡¨
                    all_qa_items.extend(new_items)
                    completed_count += len(new_items)
                    
                    logger.info(f"ç¬¬ {batch_num + 1} æ‰¹å®Œæˆï¼Œç”Ÿæˆäº† {len(new_items)} ä¸ªé—®ç­”å¯¹")
                    logger.info(f"æ€»è¿›åº¦: {completed_count}/{num_questions} ({completed_count/num_questions*100:.1f}%)")
                    
                    # æ›´æ–°è¿›åº¦
                    if progress_callback:
                        progress_callback(completed_count, num_questions, f"ç¬¬ {batch_num + 1} æ‰¹å®Œæˆ")
                    
                    # å¦‚æœå·²ç»è¾¾åˆ°ç›®æ ‡æ•°é‡ï¼Œæå‰ç»“æŸ
                    if completed_count >= num_questions:
                        break
                        
                except Exception as e:
                    logger.error(f"ç¬¬ {batch_num + 1} æ‰¹ç”Ÿæˆå¤±è´¥: {str(e)}")
                    if progress_callback:
                        progress_callback(completed_count, num_questions, f"ç¬¬ {batch_num + 1} æ‰¹å¤±è´¥: {str(e)}")
                    # ç»§ç»­ä¸‹ä¸€æ‰¹ï¼Œä¸ä¸­æ–­æ•´ä¸ªä»»åŠ¡
                    continue
            
            # å¦‚æœç”Ÿæˆçš„é—®ç­”å¯¹æ•°é‡ä¸è¶³ï¼Œç»§ç»­ç”Ÿæˆç›´åˆ°è¾¾åˆ°ç›®æ ‡æ•°é‡
            if len(all_qa_items) < num_questions:
                logger.warning(f"ç”Ÿæˆçš„é—®ç­”å¯¹æ•°é‡ä¸è¶³ ({len(all_qa_items)}/{num_questions})ï¼Œç»§ç»­ç”Ÿæˆ")
                
                if progress_callback:
                    progress_callback(completed_count, num_questions, "ç»§ç»­ç”Ÿæˆè¡¥å……æ•°æ®...")
                
                # ç»§ç»­ç”Ÿæˆç›´åˆ°è¾¾åˆ°ç›®æ ‡æ•°é‡
                while len(all_qa_items) < num_questions:
                    remaining = num_questions - len(all_qa_items)
                    current_batch_size = min(strategy['batch_size'], remaining)
                    
                    logger.info(f"ç»§ç»­ç”Ÿæˆï¼Œå‰©ä½™ {remaining} ä¸ªï¼Œæœ¬æ‰¹ç”Ÿæˆ {current_batch_size} ä¸ª")
                    
                    try:
                        # ä½¿ç”¨DeepEvalç»§ç»­ç”Ÿæˆ
                        dataset: EvaluationDataset = await self.synthesizer.a_generate_goldens_from_contexts(
                            contexts=contexts_with_instruction,
                            include_expected_output=True,
                            max_goldens_per_context=max(1, current_batch_size // len(contexts_with_instruction))
                        )
                        
                        # è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
                        batch_items = []
                        for golden in dataset:
                            qa_item = {
                                'question': golden.input,
                                'expected_output': golden.expected_output,
                                'context': contexts,
                                'context_length': sum(len(x) for x in contexts)
                            }
                            batch_items.append(qa_item)
                        
                        # å»é‡å¤„ç†
                        new_items = []
                        existing_questions = set(item['question'] for item in all_qa_items)
                        
                        for item in batch_items:
                            if item['question'] not in existing_questions:
                                new_items.append(item)
                                existing_questions.add(item['question'])
                            else:
                                logger.warning(f"å‘ç°é‡å¤é—®é¢˜ï¼Œè·³è¿‡: {item['question'][:50]}...")
                        
                        all_qa_items.extend(new_items)
                        completed_count += len(new_items)
                        
                        logger.info(f"ç»§ç»­ç”Ÿæˆå®Œæˆï¼Œæ–°å¢ {len(new_items)} ä¸ªé—®ç­”å¯¹")
                        logger.info(f"æ€»è¿›åº¦: {completed_count}/{num_questions} ({completed_count/num_questions*100:.1f}%)")
                        
                        # æ›´æ–°è¿›åº¦
                        if progress_callback:
                            progress_callback(completed_count, num_questions, f"ç»§ç»­ç”Ÿæˆå®Œæˆï¼Œæ–°å¢ {len(new_items)} ä¸ª")
                        
                        if len(new_items) == 0:
                            logger.warning("æœ¬æ‰¹æ²¡æœ‰ç”Ÿæˆæ–°çš„é—®ç­”å¯¹ï¼Œå¯èƒ½è¾¾åˆ°ç”Ÿæˆä¸Šé™")
                            break
                            
                    except Exception as e:
                        logger.error(f"ç»§ç»­ç”Ÿæˆå¤±è´¥: {str(e)}")
                        if progress_callback:
                            progress_callback(completed_count, num_questions, f"ç»§ç»­ç”Ÿæˆå¤±è´¥: {str(e)}")
                        break
            
            # åªå–éœ€è¦çš„æ•°é‡
            final_items = all_qa_items[:num_questions]
            
            logger.info(f"DeepEvalç”Ÿæˆå®Œæˆï¼Œæ€»å…±ç”Ÿæˆäº† {len(final_items)} ä¸ªé—®ç­”å¯¹")
            
            # æœ€ç»ˆè¿›åº¦æ›´æ–°
            if progress_callback:
                progress_callback(len(final_items), num_questions, "ç”Ÿæˆå®Œæˆ")
            
            return final_items
            
        except Exception as e:
            logger.error(f"DeepEvalç”Ÿæˆå¤±è´¥: {str(e)}")
            if progress_callback:
                progress_callback(0, num_questions, f"ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise
    
    async def generate_from_documents(
        self, 
        document_paths: List[str], 
        num_questions: int,
        scenario: str = "educational"
    ) -> List[Dict]:
        """
        ä»æ–‡æ¡£ç”Ÿæˆé—®ç­”å¯¹
        
        Args:
            document_paths: æ–‡æ¡£è·¯å¾„åˆ—è¡¨
            num_questions: éœ€è¦ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            scenario: ç”Ÿæˆåœºæ™¯
            
        Returns:
            ç”Ÿæˆçš„é—®ç­”å¯¹åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨DeepEvalä»æ–‡æ¡£ç”Ÿæˆæ•°æ®é›†: æ–‡æ¡£æ•°é‡={len(document_paths)}, é—®é¢˜æ•°é‡={num_questions}")
            
            # è®¡ç®—æ¯ä¸ªæ–‡æ¡£ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            max_goldens_per_context = max(1, num_questions // len(document_paths))
            
            # ä½¿ç”¨DeepEvalç”Ÿæˆæ•°æ®é›†
            dataset: EvaluationDataset = await self.synthesizer.a_generate_goldens_from_docs(
                document_paths=document_paths,
                include_expected_output=True,
                max_goldens_per_context=max_goldens_per_context
            )
            
            # è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
            qa_items = []
            for golden in dataset:
                qa_item = {
                    'question': golden.input,
                    'expected_output': golden.expected_output,
                    'context': [f"æ–‡æ¡£: {golden.source_file}"] if hasattr(golden, 'source_file') else [],
                    'context_length': len(golden.input)
                }
                qa_items.append(qa_item)
            
            logger.info(f"DeepEvalæ–‡æ¡£ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(qa_items)} ä¸ªé—®ç­”å¯¹")
            return qa_items
            
        except Exception as e:
            logger.error(f"DeepEvalæ–‡æ¡£ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise
    
    async def generate_from_scratch(
        self, 
        num_questions: int,
        scenario: str = "educational"
    ) -> List[Dict]:
        """
        ä»é›¶å¼€å§‹ç”Ÿæˆé—®ç­”å¯¹
        
        Args:
            num_questions: éœ€è¦ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            scenario: ç”Ÿæˆåœºæ™¯
            
        Returns:
            ç”Ÿæˆçš„é—®ç­”å¯¹åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨DeepEvalä»é›¶ç”Ÿæˆæ•°æ®é›†: é—®é¢˜æ•°é‡={num_questions}")
            
            # ä½¿ç”¨DeepEvalç”Ÿæˆæ•°æ®é›†
            dataset: EvaluationDataset = await self.synthesizer.a_generate_goldens_from_scratch(
                num_goldens=num_questions
            )
            
            # è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
            qa_items = []
            for golden in dataset:
                qa_item = {
                    'question': golden.input,
                    'expected_output': golden.expected_output,
                    'context': [],
                    'context_length': 0
                }
                qa_items.append(qa_item)
            
            logger.info(f"DeepEvalä»é›¶ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(qa_items)} ä¸ªé—®ç­”å¯¹")
            return qa_items
            
        except Exception as e:
            logger.error(f"DeepEvalä»é›¶ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise
    
    def save_dataset(self, qa_items: List[Dict], file_path: str, file_type: str = 'csv'):
        """
        ä¿å­˜æ•°æ®é›†åˆ°æ–‡ä»¶
        
        Args:
            qa_items: é—®ç­”å¯¹åˆ—è¡¨
            file_path: æ–‡ä»¶è·¯å¾„
            file_type: æ–‡ä»¶ç±»å‹ (csv, json, jsonl)
        """
        try:
            # åˆ›å»ºä¸´æ—¶æ•°æ®é›†ç”¨äºä¿å­˜
            temp_dataset = []
            for item in qa_items:
                from deepeval.dataset import Golden
                golden = Golden(
                    input=item['question'],
                    expected_output=item['expected_output']
                )
                temp_dataset.append(golden)
            
            # ä½¿ç”¨DeepEvalçš„ä¿å­˜åŠŸèƒ½
            self.synthesizer.synthetic_goldens = temp_dataset
            saved_path = self.synthesizer.save_as(
                file_type=file_type,
                directory=os.path.dirname(file_path),
                file_name=os.path.basename(file_path).split('.')[0],
                quiet=True
            )
            
            logger.info(f"æ•°æ®é›†å·²ä¿å­˜åˆ°: {saved_path}")
            return saved_path
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®é›†å¤±è´¥: {str(e)}")
            raise

# å…¨å±€å®ä¾‹
deepeval_generator = DeepEvalDatasetGenerator() 