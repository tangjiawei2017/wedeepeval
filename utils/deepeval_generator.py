import os
import asyncio
import sys
from typing import List, Dict, Optional, Callable

# è·å–é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DEEPEVAL_SOURCE_PATH = os.path.join(PROJECT_ROOT, 'deepeval_source')

# æ·»åŠ æœ¬åœ° DeepEval æºç è·¯å¾„åˆ° Python è·¯å¾„çš„æœ€å‰é¢
if DEEPEVAL_SOURCE_PATH not in sys.path:
    sys.path.insert(0, DEEPEVAL_SOURCE_PATH)
    print(f"ğŸ”§ å·²æ·»åŠ æœ¬åœ° DeepEval æºç è·¯å¾„: {DEEPEVAL_SOURCE_PATH}")

# éªŒè¯å¯¼å…¥è·¯å¾„
try:
    import deepeval
    print(f"âœ… æˆåŠŸå¯¼å…¥ DeepEvalï¼Œè·¯å¾„: {deepeval.__file__}")
    if 'deepeval_source' in deepeval.__file__:
        print("ğŸ¯ ä½¿ç”¨çš„æ˜¯æœ¬åœ° DeepEval æºç ")
    else:
        print("âš ï¸  è­¦å‘Šï¼šä½¿ç”¨çš„æ˜¯ç³»ç»Ÿå®‰è£…çš„ DeepEval åŒ…")
except ImportError as e:
    print(f"âŒ DeepEval å¯¼å…¥å¤±è´¥: {e}")
    raise

from deepeval.synthesizer import Synthesizer
from deepeval.synthesizer.config import StylingConfig
from deepeval.dataset import EvaluationDataset
from utils.logger import get_logger
from config import API_CONFIG, GENERATION_CONFIG

# è·å–æ—¥å¿—è®°å½•å™¨
logger = get_logger('deepeval_generator')

class DeepEvalDatasetGenerator:
    """ä½¿ç”¨DeepEvalç”Ÿæˆæ•°æ®é›†çš„å·¥å…·ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–DeepEvalæ•°æ®é›†ç”Ÿæˆå™¨"""
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ["OPENAI_API_KEY"] = API_CONFIG['openai_api_key']
        os.environ["OPENAI_BASE_URL"] = API_CONFIG['openai_base_url']
        
        # åˆå§‹åŒ–Synthesizer
        self.synthesizer = Synthesizer(
            model=API_CONFIG['openai_model'],
            async_mode=True,
            max_concurrent=10
        )
        
        logger.info(f"DeepEvalæ•°æ®é›†ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹: {API_CONFIG['openai_model']}")
    
    def _prepare_contexts_with_instruction(self, contexts: List[str]) -> List[List[str]]:
        """ä¸ºä¸Šä¸‹æ–‡æ·»åŠ æ¨ç†æŒ‡ç¤º"""
        instruction = "è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯è¿›è¡Œæ¨ç†ï¼Œç”Ÿæˆä¸­æ–‡é—®é¢˜å’Œç­”æ¡ˆï¼š"
        contexts_with_instruction = [instruction] + contexts
        return [contexts_with_instruction]
    
    def _create_styling_config(self, scenario: str = "educational") -> StylingConfig:
        """åˆ›å»ºç”Ÿæˆé£æ ¼é…ç½®"""
        return StylingConfig(
            scenario=scenario,  # æ•™è‚²åœºæ™¯
            task="question_generation",  # é—®é¢˜ç”Ÿæˆä»»åŠ¡
            input_format="Chinese",  # è¾“å…¥æ ¼å¼ä¸ºä¸­æ–‡
            expected_output_format="Chinese"  # æœŸæœ›è¾“å‡ºæ ¼å¼ä¸ºä¸­æ–‡
        )
    
    def _calculate_batch_strategy(self, num_questions: int) -> Dict:
        """ä¿æŒå…¼å®¹ä½†ä¸å†ä½¿ç”¨åˆ†æ‰¹ï¼Œç»Ÿä¸€è¿”å›å•æ‰¹å…¨é‡ã€‚"""
        return {
            'strategy': 'single_batch',
            'batch_size': num_questions,
            'num_batches': 1
        }
    
    async def generate_from_contexts(
        self, 
        contexts: List[str], 
        num_questions: int,
        scenario: str = "educational",
        progress_callback: Optional[Callable[[int, int, str], None]] = None
    ) -> List[Dict]:
        """
        ä»ä¸Šä¸‹æ–‡ç”Ÿæˆé—®ç­”å¯¹ï¼Œæ”¯æŒåˆ†æ‰¹ç”Ÿæˆå’Œè¿›åº¦è·Ÿè¸ª
        
        Args:
            contexts: ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ—è¡¨
            num_questions: éœ€è¦ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            scenario: ç”Ÿæˆåœºæ™¯ (educational, conversational, technical)
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (completed, total, status)
            
        Returns:
            ç”Ÿæˆçš„é—®ç­”å¯¹åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨DeepEvalç”Ÿæˆæ•°æ®é›†: ä¸Šä¸‹æ–‡æ•°é‡={len(contexts)}, é—®é¢˜æ•°é‡={num_questions}")

            # ä¸€æ¬¡æ€§å…¨é‡ç”Ÿæˆï¼ˆæ— åˆ†æ‰¹ï¼‰
            contexts_with_instruction = self._prepare_contexts_with_instruction(contexts)
            styling_config = self._create_styling_config(scenario)

            if progress_callback:
                progress_callback(0, num_questions, "å¼€å§‹å…¨é‡ç”Ÿæˆ...")

            # æŒ‚è½½å¤–éƒ¨è¿›åº¦é’©å­ï¼Œå°† rich è¿›åº¦æ›´æ–°é€ä¼ åˆ°å›è°ƒ
            prev_hook = None
            try:
                import deepeval.utils as dutils

                prev_hook = getattr(dutils, 'EXTERNAL_PROGRESS_HOOK', None)

                def _hook(event: dict):
                    try:
                        desc = (event.get('description') or '').lower()
                        # åªæ‹¦æˆªä¸»è¿›åº¦æ¡ï¼Œé¿å…åŠ è½½/åˆ‡åˆ†ç­‰å­ä»»åŠ¡å¹²æ‰°
                        if not (
                            'generating goldens from context' in desc
                            or 'generating input' in desc
                            or 'generate goldens' in desc
                        ):
                            return
                        raw_completed = int(event.get('completed') or 0)
                        raw_total = int((event.get('total') or 0) or 1)
                        ratio = max(0.0, min(1.0, (raw_completed / raw_total)))
                        mapped_completed = int(min(num_questions, max(0, int(ratio * num_questions))))
                        status = event.get('description') or "ç”Ÿæˆä¸­"
                        logger.info(f"[progress_hook] {status} raw={raw_completed}/{raw_total} -> mapped={mapped_completed}/{num_questions}")
                        if progress_callback:
                            progress_callback(mapped_completed, num_questions, status)
                    except Exception:
                        pass

                dutils.EXTERNAL_PROGRESS_HOOK = _hook

                max_goldens_per_context = max(1, num_questions)
                dataset: EvaluationDataset = await self.synthesizer.a_generate_goldens_from_contexts(
                    contexts=contexts_with_instruction,
                    include_expected_output=True,
                    max_goldens_per_context=max_goldens_per_context
                )
            finally:
                try:
                    import deepeval.utils as dutils
                    dutils.EXTERNAL_PROGRESS_HOOK = prev_hook
                except Exception:
                    pass

            qa_items = []
            seen = set()
            for golden in dataset:
                q = golden.input
                if q in seen:
                    continue
                seen.add(q)
                qa_items.append({
                    'question': q,
                    'expected_output': golden.expected_output,
                    'context': contexts,
                    'context_length': sum(len(x) for x in contexts)
                })

            final_items = qa_items[:num_questions]
            logger.info(f"DeepEvalç”Ÿæˆå®Œæˆï¼Œæ€»å…±ç”Ÿæˆäº† {len(final_items)} ä¸ªé—®ç­”å¯¹")

            if progress_callback:
                progress_callback(len(final_items), num_questions, "ç”Ÿæˆå®Œæˆ")

            return final_items
            
        except Exception as e:
            logger.error(f"DeepEvalç”Ÿæˆå¤±è´¥: {str(e)}")
            if progress_callback:
                progress_callback(0, num_questions, f"ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise
    
    async def generate_from_documents(
        self, 
        document_paths: List[str], 
        num_questions: int,
        scenario: str = "educational"
    ) -> List[Dict]:
        """
        ä»æ–‡æ¡£ç”Ÿæˆé—®ç­”å¯¹
        
        Args:
            document_paths: æ–‡æ¡£è·¯å¾„åˆ—è¡¨
            num_questions: éœ€è¦ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            scenario: ç”Ÿæˆåœºæ™¯
            
        Returns:
            ç”Ÿæˆçš„é—®ç­”å¯¹åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨DeepEvalä»æ–‡æ¡£ç”Ÿæˆæ•°æ®é›†: æ–‡æ¡£æ•°é‡={len(document_paths)}, é—®é¢˜æ•°é‡={num_questions}")
            
            # è®¡ç®—æ¯ä¸ªæ–‡æ¡£ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            max_goldens_per_context = max(1, num_questions // len(document_paths))
            
            # ä½¿ç”¨DeepEvalç”Ÿæˆæ•°æ®é›†
            dataset: EvaluationDataset = await self.synthesizer.a_generate_goldens_from_docs(
                document_paths=document_paths,
                include_expected_output=True,
                max_goldens_per_context=max_goldens_per_context
            )
            
            # è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
            qa_items = []
            for golden in dataset:
                qa_item = {
                    'question': golden.input,
                    'expected_output': golden.expected_output,
                    'context': [f"æ–‡æ¡£: {golden.source_file}"] if hasattr(golden, 'source_file') else [],
                    'context_length': len(golden.input)
                }
                qa_items.append(qa_item)
            
            logger.info(f"DeepEvalæ–‡æ¡£ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(qa_items)} ä¸ªé—®ç­”å¯¹")
            return qa_items
            
        except Exception as e:
            logger.error(f"DeepEvalæ–‡æ¡£ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise
    
    async def generate_from_scratch(
        self, 
        num_questions: int,
        scenario: str = "educational"
    ) -> List[Dict]:
        """
        ä»é›¶å¼€å§‹ç”Ÿæˆé—®ç­”å¯¹
        
        Args:
            num_questions: éœ€è¦ç”Ÿæˆçš„é—®é¢˜æ•°é‡
            scenario: ç”Ÿæˆåœºæ™¯
            
        Returns:
            ç”Ÿæˆçš„é—®ç­”å¯¹åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹ä½¿ç”¨DeepEvalä»é›¶ç”Ÿæˆæ•°æ®é›†: é—®é¢˜æ•°é‡={num_questions}")
            
            # ä½¿ç”¨DeepEvalç”Ÿæˆæ•°æ®é›†
            dataset: EvaluationDataset = await self.synthesizer.a_generate_goldens_from_scratch(
                num_goldens=num_questions
            )
            
            # è½¬æ¢ä¸ºæˆ‘ä»¬çš„æ ¼å¼
            qa_items = []
            for golden in dataset:
                qa_item = {
                    'question': golden.input,
                    'expected_output': golden.expected_output,
                    'context': [],
                    'context_length': 0
                }
                qa_items.append(qa_item)
            
            logger.info(f"DeepEvalä»é›¶ç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(qa_items)} ä¸ªé—®ç­”å¯¹")
            return qa_items
            
        except Exception as e:
            logger.error(f"DeepEvalä»é›¶ç”Ÿæˆå¤±è´¥: {str(e)}")
            raise
    
    def save_dataset(self, qa_items: List[Dict], file_path: str, file_type: str = 'csv'):
        """
        ä¿å­˜æ•°æ®é›†åˆ°æ–‡ä»¶
        
        Args:
            qa_items: é—®ç­”å¯¹åˆ—è¡¨
            file_path: æ–‡ä»¶è·¯å¾„
            file_type: æ–‡ä»¶ç±»å‹ (csv, json, jsonl)
        """
        try:
            # åˆ›å»ºä¸´æ—¶æ•°æ®é›†ç”¨äºä¿å­˜
            temp_dataset = []
            for item in qa_items:
                from deepeval.dataset import Golden
                golden = Golden(
                    input=item['question'],
                    expected_output=item['expected_output']
                )
                temp_dataset.append(golden)
            
            # ä½¿ç”¨DeepEvalçš„ä¿å­˜åŠŸèƒ½
            self.synthesizer.synthetic_goldens = temp_dataset
            saved_path = self.synthesizer.save_as(
                file_type=file_type,
                directory=os.path.dirname(file_path),
                file_name=os.path.basename(file_path).split('.')[0],
                quiet=True
            )
            
            logger.info(f"æ•°æ®é›†å·²ä¿å­˜åˆ°: {saved_path}")
            return saved_path
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®é›†å¤±è´¥: {str(e)}")
            raise

# å…¨å±€å®ä¾‹
deepeval_generator = DeepEvalDatasetGenerator() 